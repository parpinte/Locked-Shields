{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T13:16:58.866616Z",
     "start_time": "2025-01-20T13:16:58.865284Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:26:35.809823Z",
     "start_time": "2025-01-20T18:26:35.752966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.signal import correlate\n",
    "\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, filepath= None):\n",
    "        self.filepath = filepath\n",
    "        self.y = None\n",
    "        self.log = None\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load dataset from a CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.data = pd.read_csv(self.filepath, low_memory=False)\n",
    "            print(\"Dataset loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: File not found. Please check the file path.\")\n",
    "\n",
    "\n",
    "    def show_features(self):\n",
    "        \"\"\" Display the features of the data. \"\"\"\n",
    "        if self.data is not None:\n",
    "            print(\"Features in the dataset:\")\n",
    "            feature_list = self.data.columns.tolist()\n",
    "            for i, feature in enumerate(feature_list, 1):\n",
    "                print(f\"{i}. {feature}\")\n",
    "            print(f\"\\nTotal features: {len(feature_list)}\")\n",
    "        else:\n",
    "            print(\"No data loaded. Please load a dataset first.\")\n",
    "\n",
    "    def drop_features(self, columns_to_drop):\n",
    "        \"\"\"\n",
    "        Drop specific columns from the data.\n",
    "        columns_to_drop (list): List of column names to be dropped\n",
    "        Tip: Use the `show_features` function to check the available features in the dataset.\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            existing_columns = set(self.data.columns)\n",
    "            columns_to_drop = set(columns_to_drop)\n",
    "            valid_columns = columns_to_drop.intersection(existing_columns)\n",
    "            missing_columns = columns_to_drop - valid_columns\n",
    "\n",
    "            if valid_columns:\n",
    "                self.data.drop(valid_columns, axis=1, inplace=True)\n",
    "                print(f\"Successfully dropped the following columns: {', '.join(valid_columns)}\")\n",
    "            if missing_columns:\n",
    "                print(f\"Warning: The following columns were not found in the dataset and were ignored: {', '.join(missing_columns)}\")\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "\n",
    "    def extract_target(self, target_column):\n",
    "        \"\"\"\n",
    "        Extract the target column, store it in self.y, and remove it from the dataset.\n",
    "        Args: target_column (str): Name of the target column to extract.\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            if target_column in self.data.columns:\n",
    "                self.y = self.data[target_column]\n",
    "                self.data.drop(target_column, axis=1, inplace=True)\n",
    "                print(f\"Target column '{target_column}' extracted successfully and stored in self.y.\")\n",
    "            else:\n",
    "                print(f\"Error: Column '{target_column}' not found in the dataset.\")\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "\n",
    "    def detect_categorical(self, handle_nan=\"unknown\"):\n",
    "        \"\"\"\n",
    "        Detects categorical features in the dataset and identifies NaN values.\n",
    "        Handles NaN values in the categorical features based on the chosen method.\n",
    "        Ensures target variable is updated if rows are dropped.\n",
    "\n",
    "        Args:\n",
    "            handle_nan (str): How to handle NaN values in categorical features.\n",
    "                              Options are \"drop\", \"most_frequent\", or \"unknown\".\n",
    "        \"\"\"\n",
    "        if self.data is not None and self.y is not None:\n",
    "            # Detect categorical features\n",
    "            categorical_features = self.data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "            if len(categorical_features) == 0:\n",
    "                print(\"No categorical features detected.\")\n",
    "                return\n",
    "\n",
    "            print(\"Categorical Features and their NaN Information:\")\n",
    "            for feature in categorical_features:\n",
    "                total_nan = self.data[feature].isna().sum()\n",
    "                percentage_nan = (total_nan / len(self.data)) * 100\n",
    "\n",
    "                print(f\"- {feature}:\")\n",
    "                print(f\"  NaN Count: {total_nan}\")\n",
    "                print(f\"  Percentage of NaNs: {percentage_nan:.2f}%\")\n",
    "\n",
    "                # Handle NaN values based on the chosen option\n",
    "                if total_nan > 0:  # Only handle if there are NaN values\n",
    "                    if handle_nan == \"drop\":\n",
    "                        # Identify rows to keep\n",
    "                        rows_to_keep = ~self.data[feature].isna()\n",
    "                        self.data = self.data[rows_to_keep]\n",
    "                        self.y = self.y[rows_to_keep]\n",
    "                        print(f\"  Action: Dropped rows with NaN in '{feature}'.\")\n",
    "\n",
    "                    elif handle_nan == \"most_frequent\":\n",
    "                        most_frequent = self.data[feature].mode()[0]\n",
    "                        self.data[feature] = self.data[feature].fillna(most_frequent)\n",
    "                        print(f\"  Action: Replaced NaN with most frequent value '{most_frequent}'.\")\n",
    "\n",
    "                    elif handle_nan == \"unknown\":\n",
    "                        self.data[feature] = self.data[feature].fillna(\"Unknown\")\n",
    "                        print(f\"  Action: Replaced NaN with 'Unknown'.\")\n",
    "\n",
    "                    else:\n",
    "                        print(f\"  Action: Invalid option '{handle_nan}'. No changes made for '{feature}'.\")\n",
    "\n",
    "            print(\"\\nCategorical NaN handling completed.\")\n",
    "        else:\n",
    "            print(\"No data loaded or target variable not set. Please load the data and ensure target is separated.\")\n",
    "\n",
    "\n",
    "    def handle_duplicates(self):\n",
    "        \"\"\"\n",
    "        Check for and remove duplicate rows in the dataset.\n",
    "        Ensures the corresponding values in the target variable are also removed.\n",
    "        \"\"\"\n",
    "        if self.data is not None and self.y is not None:\n",
    "            # Find duplicate rows\n",
    "            duplicates = self.data.duplicated()\n",
    "            num_duplicates = duplicates.sum()\n",
    "\n",
    "            if num_duplicates > 0:\n",
    "                print(f\"Found {num_duplicates} duplicate rows in the dataset.\")\n",
    "\n",
    "                # Remove duplicate rows and corresponding target values\n",
    "                non_duplicates = ~duplicates\n",
    "                self.data = self.data[non_duplicates]\n",
    "                self.y = self.y[non_duplicates]\n",
    "\n",
    "                print(\"Duplicate rows have been removed.\")\n",
    "            else:\n",
    "                print(\"No duplicate rows found in the dataset.\")\n",
    "        else:\n",
    "            print(\"No data loaded or target variable not set. Please load the data and ensure the target is separated.\")\n",
    "\n",
    "\n",
    "    def encode_categorical(self, method=\"label\"):\n",
    "        \"\"\"\n",
    "        Encode categorical features in the dataset using the specified method.\n",
    "\n",
    "        Args:\n",
    "            method (str): Encoding method to use. Options are \"label\" (Label Encoding) or \"onehot\" (One-Hot Encoding).\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            # Detect categorical features\n",
    "            categorical_features = self.data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "            if len(categorical_features) == 0:\n",
    "                print(\"No categorical features to encode.\")\n",
    "                return\n",
    "\n",
    "            print(f\"Encoding categorical features using {method} encoding.\")\n",
    "\n",
    "            if method == \"label\":\n",
    "                for feature in categorical_features:\n",
    "                    le = LabelEncoder()\n",
    "                    self.data[feature] = le.fit_transform(self.data[feature])\n",
    "                    print(f\"Feature '{feature}' encoded using Label Encoding.\")\n",
    "\n",
    "            elif method == \"onehot\":\n",
    "                self.data = pd.get_dummies(self.data, columns=categorical_features, drop_first=True)\n",
    "                print(f\"Features encoded using One-Hot Encoding.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Error: Invalid encoding method '{method}'. Choose 'label' or 'onehot'.\")\n",
    "\n",
    "            print(\"Categorical encoding completed.\")\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "\n",
    "    def study_correlation(self, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Compute the correlation matrix, identify highly correlated features, and return a figure with only those features.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): The correlation coefficient threshold for identifying highly correlated features.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tuples containing pairs of features with correlation above the threshold.\n",
    "            plt.Figure: The filtered correlation matrix heatmap figure.\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            # Compute the correlation matrix\n",
    "            correlation_matrix = self.data.corr()\n",
    "\n",
    "            # Identify highly correlated features\n",
    "            correlated_features = []\n",
    "            features_to_include = set()\n",
    "            for i in range(correlation_matrix.shape[0]):\n",
    "                for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "                    if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                        correlated_features.append((correlation_matrix.index[i], correlation_matrix.columns[j]))\n",
    "                        features_to_include.update([correlation_matrix.index[i], correlation_matrix.columns[j]])\n",
    "\n",
    "            # Filter the correlation matrix to only include highly correlated features\n",
    "            filtered_features = list(features_to_include)\n",
    "            filtered_correlation_matrix = correlation_matrix.loc[filtered_features, filtered_features]\n",
    "\n",
    "            # Plot the filtered heatmap\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(filtered_correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "            plt.title(f\"Filtered Correlation Matrix (Threshold > {threshold})\")\n",
    "            plt.tight_layout()\n",
    "            fig = plt.gcf()\n",
    "\n",
    "            # Print highly correlated features\n",
    "            if correlated_features:\n",
    "                print(\"Highly correlated features (absolute correlation > threshold):\")\n",
    "                for pair in correlated_features:\n",
    "                    print(f\"{pair[0]} â†” {pair[1]}\")\n",
    "            else:\n",
    "                print(\"No features are highly correlated based on the threshold.\")\n",
    "\n",
    "            return correlated_features, fig\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "            return None, None\n",
    "\n",
    "    def drop_highly_correlated(self, correlated_features):\n",
    "        \"\"\"\n",
    "        Drop one feature from each pair of highly correlated features, excluding self-correlations.\n",
    "        Args: correlated_features (list): List of tuples containing pairs of highly correlated features\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            if not correlated_features:\n",
    "                print(\"No highly correlated features to drop.\")\n",
    "                return\n",
    "\n",
    "            # Keep track of dropped features to avoid redundancy\n",
    "            dropped_features = set()\n",
    "\n",
    "            for feature1, feature2 in correlated_features:\n",
    "                # Avoid self-correlations\n",
    "                if feature1 != feature2:\n",
    "                    # Drop the second feature in the pair if not already dropped\n",
    "                    if feature2 not in dropped_features:\n",
    "                        self.data.drop(columns=[feature2], inplace=True)\n",
    "                        dropped_features.add(feature2)\n",
    "                        print(f\"Dropped feature: {feature2} (correlated with {feature1})\")\n",
    "\n",
    "            print(\"Highly correlated features have been addressed.\")\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "\n",
    "    def apply_pca(self, n_components=None, plot_variance=False):\n",
    "        \"\"\"\n",
    "        Apply PCA for dimensionality reduction.\n",
    "\n",
    "        Args:\n",
    "            n_components (int or float): Number of principal components to keep.\n",
    "                                         If float (0 < n_components <= 1), it represents the variance ratio to preserve.\n",
    "                                         If None, keep all components.\n",
    "            plot_variance (bool): If True, plots the explained variance ratio for each component.\n",
    "\n",
    "        Returns: Transformed dataset with reduced dimensions.\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            # Ensure only numeric data is used for PCA\n",
    "            numeric_data = self.data.select_dtypes(include=[\"number\"])\n",
    "\n",
    "            if numeric_data.empty:\n",
    "                print(\"No numeric data available for PCA.\")\n",
    "                return None\n",
    "\n",
    "            # Initialize PCA\n",
    "            pca = PCA(n_components=n_components)\n",
    "            reduced_data = pca.fit_transform(numeric_data)\n",
    "\n",
    "            # Create a DataFrame for the reduced data\n",
    "            reduced_df = pd.DataFrame(\n",
    "                reduced_data,\n",
    "                columns=[f\"PC{i+1}\" for i in range(reduced_data.shape[1])]\n",
    "            )\n",
    "            print(f\"PCA applied. Reduced dataset shape: {reduced_df.shape}\")\n",
    "\n",
    "            # Optionally plot explained variance ratio\n",
    "            if plot_variance:\n",
    "                plt.figure(figsize=(8, 5))\n",
    "                plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "                         pca.explained_variance_ratio_.cumsum(), marker=\"o\")\n",
    "                plt.title(\"Cumulative Explained Variance by Principal Components\")\n",
    "                plt.xlabel(\"Number of Principal Components\")\n",
    "                plt.ylabel(\"Cumulative Explained Variance\")\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "\n",
    "            return reduced_df\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "            return None\n",
    "\n",
    "    def combine_and_replace_correlated_features(self, correlated_features, method=\"mean\"):\n",
    "        \"\"\"\n",
    "        Combine pairs of correlated features by taking their mean or maximum,\n",
    "        and replace the original features with the new combined features.\n",
    "\n",
    "        Args:\n",
    "            correlated_features (list): List of tuples containing pairs of highly correlated features.\n",
    "            method (str): Method to combine the features. Options are \"mean\" or \"max\".\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            if not correlated_features:\n",
    "                print(\"No highly correlated features to combine.\")\n",
    "                return\n",
    "\n",
    "            # Track processed features to ensure proper replacement\n",
    "            processed_features = set()\n",
    "\n",
    "            for feature1, feature2 in correlated_features:\n",
    "                # Avoid self-correlations and redundant processing\n",
    "                if feature1 != feature2 and (feature1, feature2) not in processed_features and (feature2, feature1) not in processed_features:\n",
    "                    # Check if both features exist in the dataset\n",
    "                    if feature1 in self.data.columns and feature2 in self.data.columns:\n",
    "                        # Combine the features\n",
    "                        if method == \"mean\":\n",
    "                            self.data[f\"{feature1}_{feature2}_combined\"] = self.data[[feature1, feature2]].mean(axis=1)\n",
    "                        elif method == \"max\":\n",
    "                            self.data[f\"{feature1}_{feature2}_combined\"] = self.data[[feature1, feature2]].max(axis=1)\n",
    "                        else:\n",
    "                            print(f\"Invalid method '{method}'. Use 'mean' or 'max'.\")\n",
    "                            return\n",
    "\n",
    "                        # Remove the original features\n",
    "                        self.data.drop(columns=[feature1, feature2], inplace=True)\n",
    "                        print(f\"Replaced '{feature1}' and '{feature2}' with '{feature1}_{feature2}_combined'.\")\n",
    "\n",
    "                        # Mark the pair as processed\n",
    "                        processed_features.add((feature1, feature2))\n",
    "                    else:\n",
    "                        print(f\"Skipped combination for '{feature1}' and '{feature2}' as one or both are missing in the dataset.\")\n",
    "\n",
    "            print(\"Correlated feature replacement completed. Dataset dimensions reduced.\")\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "\n",
    "\n",
    "    def select_features_by_importance(self, threshold=0.01, model=None):\n",
    "        \"\"\"\n",
    "        Select features based on their importance scores.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): Minimum importance score for a feature to be selected.\n",
    "            model: Pre-trained model with `feature_importances_` attribute.\n",
    "                   If None, a RandomForestClassifier is used.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dataset with selected features only.\n",
    "        \"\"\"\n",
    "        if self.data is not None and self.y is not None:\n",
    "            # Ensure only numeric data is used for feature selection\n",
    "            numeric_data = self.data.select_dtypes(include=[\"number\"])\n",
    "\n",
    "            if numeric_data.empty:\n",
    "                print(\"No numeric features available for feature selection.\")\n",
    "                return None\n",
    "\n",
    "            # Default to a RandomForestClassifier if no model is provided\n",
    "            if model is None:\n",
    "                model = RandomForestClassifier(random_state=42)\n",
    "                model.fit(numeric_data, self.y)\n",
    "\n",
    "            # Check if the model has the feature_importances_ attribute\n",
    "            if not hasattr(model, \"feature_importances_\"):\n",
    "                print(\"The provided model does not support feature importance scoring.\")\n",
    "                return None\n",
    "\n",
    "            # Get feature importances\n",
    "            feature_importances = model.feature_importances_\n",
    "            important_features = numeric_data.columns[feature_importances >= threshold]\n",
    "\n",
    "            print(\"Selected Features Based on Importance:\")\n",
    "            for feature in important_features:\n",
    "                print(f\"- {feature} (Importance: {feature_importances[numeric_data.columns.get_loc(feature)]:.4f})\")\n",
    "\n",
    "            # Reduce the dataset to only the selected features\n",
    "            self.data = self.data[important_features]\n",
    "            print(f\"Feature selection completed. Reduced dataset shape: {self.data.shape}\")\n",
    "\n",
    "            return self.data\n",
    "        else:\n",
    "            print(\"No data or target variable loaded. Please load the data and ensure the target is separated.\")\n",
    "            return None\n",
    "\n",
    "    def detect_missing_numerical(self):\n",
    "        \"\"\"\n",
    "        Detect missing values (NaN and inf) in numerical features of the dataset.\n",
    "\n",
    "        Prints the number and percentage of missing values for each numerical feature.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of feature names with missing values (NaN or inf).\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            # Select numerical features\n",
    "            numeric_features = self.data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "            if len(numeric_features) == 0:\n",
    "                print(\"No numerical features detected.\")\n",
    "                return []\n",
    "\n",
    "            missing_features = []\n",
    "\n",
    "            print(\"Missing values for numerical features:\")\n",
    "            for feature in numeric_features:\n",
    "                # Count NaN values\n",
    "                nan_count = self.data[feature].isna().sum()\n",
    "\n",
    "                # Count inf values\n",
    "                inf_count = np.isinf(self.data[feature]).sum()\n",
    "\n",
    "                if nan_count > 0 or inf_count > 0:\n",
    "                    # Add feature to the list if it has missing values\n",
    "                    missing_features.append(feature)\n",
    "\n",
    "                    # Calculate percentages\n",
    "                    total_rows = len(self.data)\n",
    "                    nan_percentage = (nan_count / total_rows) * 100\n",
    "                    inf_percentage = (inf_count / total_rows) * 100\n",
    "\n",
    "                    # Print information\n",
    "                    print(f\"- {feature}:\")\n",
    "                    print(f\"  NaN Count: {nan_count} ({nan_percentage:.2f}%)\")\n",
    "                    print(f\"  Inf Count: {inf_count} ({inf_percentage:.2f}%)\")\n",
    "\n",
    "            if not missing_features:\n",
    "                print(\"No missing values detected in numerical features.\")\n",
    "\n",
    "            return missing_features\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def handle_missing_values(self, features, method=\"mean\", custom_value=None):\n",
    "        \"\"\"\n",
    "        Handle missing values (NaN and inf) in the specified numerical features.\n",
    "\n",
    "        Args:\n",
    "            features (list): List of feature names with missing values to be treated.\n",
    "            method (str): Method to handle missing values. Options are:\n",
    "                          - \"drop\": Drop rows with missing values in the specified features.\n",
    "                          - \"mean\": Replace missing values with the mean of the feature.\n",
    "                          - \"custom\": Replace missing values with a custom value.\n",
    "            custom_value (float): The value to replace missing values if method is \"custom\".\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.data is not None and self.y is not None:\n",
    "            if not features:\n",
    "                print(\"No features provided for handling missing values.\")\n",
    "                return\n",
    "\n",
    "            for feature in features:\n",
    "                if feature not in self.data.columns:\n",
    "                    print(f\"Feature '{feature}' not found in the dataset. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Handle based on the specified method\n",
    "                if method == \"drop\":\n",
    "                    rows_to_keep = ~self.data[feature].isna() & ~np.isinf(self.data[feature])\n",
    "                    print(f\"Dropping rows with missing values in '{feature}'.\")\n",
    "                    self.data = self.data[rows_to_keep]\n",
    "                    self.y = self.y[rows_to_keep]\n",
    "\n",
    "                elif method == \"mean\":\n",
    "                    mean_value = self.data[feature][~self.data[feature].isna() & ~np.isinf(self.data[feature])].mean()\n",
    "                    self.data[feature] = self.data[feature].replace([np.inf, -np.inf], np.nan).fillna(mean_value)\n",
    "                    print(f\"Replaced missing values in '{feature}' with the mean ({mean_value:.4f}).\")\n",
    "\n",
    "                elif method == \"custom\":\n",
    "                    if custom_value is None:\n",
    "                        print(f\"Custom value not provided for '{feature}'. Skipping.\")\n",
    "                        continue\n",
    "                    self.data[feature] = self.data[feature].replace([np.inf, -np.inf], np.nan).fillna(custom_value)\n",
    "                    print(f\"Replaced missing values in '{feature}' with custom value ({custom_value}).\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Invalid method '{method}' specified. Use 'drop', 'mean', or 'custom'.\")\n",
    "                    return\n",
    "\n",
    "            print(\"Missing value handling completed.\")\n",
    "        else:\n",
    "            print(\"No data or target variable loaded. Please load the data and ensure the target is separated.\")\n",
    "\n",
    "\n",
    "\n",
    "    def rescale_data(self, method=\"standardize\"):\n",
    "        \"\"\"\n",
    "        Rescale numerical features in the dataset using standardization or normalization.\n",
    "\n",
    "        Args:\n",
    "            method (str): Rescaling method. Options are:\n",
    "                          - \"standardize\": Standardize the data (mean=0, std=1).\n",
    "                          - \"normalize\": Normalize the data (min=0, max=1).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            # Ensure only numeric data is rescaled\n",
    "            numeric_features = self.data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "            if len(numeric_features) == 0:\n",
    "                print(\"No numerical features detected for rescaling.\")\n",
    "                return\n",
    "\n",
    "            # Select the rescaling method\n",
    "            if method == \"standardize\":\n",
    "                scaler = StandardScaler()\n",
    "                print(\"Applying standardization (mean=0, std=1).\")\n",
    "            elif method == \"normalize\":\n",
    "                scaler = MinMaxScaler()\n",
    "                print(\"Applying normalization (min=0, max=1).\")\n",
    "            else:\n",
    "                print(f\"Invalid method '{method}'. Use 'standardize' or 'normalize'.\")\n",
    "                return\n",
    "\n",
    "            # Apply the scaler and update the dataset\n",
    "            self.data[numeric_features] = scaler.fit_transform(self.data[numeric_features])\n",
    "            print(f\"Rescaling completed using {method}.\")\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "\n",
    "\n",
    "    def summarize_feature_distribution(self, top_categories=5):\n",
    "        \"\"\"\n",
    "        Summarize the distribution of features in the dataset.\n",
    "        For numerical features, provides summary statistics (mean, std, min, max).\n",
    "        For categorical features, lists the top categories by count.\n",
    "        Args:top_categories (int): Number of top categories to display for categorical features.\n",
    "        Returns: dict: A summary dictionary containing information about numerical and categorical features.\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            summary = {\"numerical\": {}, \"categorical\": {}}\n",
    "\n",
    "            for feature in self.data.columns:\n",
    "                # Numerical features\n",
    "                if self.data[feature].dtype in [\"int64\", \"float64\"]:\n",
    "                    stats = self.data[feature].describe()\n",
    "                    summary[\"numerical\"][feature] = {\n",
    "                        \"mean\": stats[\"mean\"],\n",
    "                        \"std\": stats[\"std\"],\n",
    "                        \"min\": stats[\"min\"],\n",
    "                        \"max\": stats[\"max\"],\n",
    "                    }\n",
    "\n",
    "                # Categorical features\n",
    "                elif self.data[feature].dtype == \"object\" or self.data[feature].dtype.name == \"category\":\n",
    "                    value_counts = self.data[feature].value_counts().head(top_categories)\n",
    "                    summary[\"categorical\"][feature] = value_counts.to_dict()\n",
    "\n",
    "            # Print the summary in a readable format\n",
    "            print(\"Summary of Feature Distribution:\")\n",
    "            print(\"\\nNumerical Features:\")\n",
    "            for feature, stats in summary[\"numerical\"].items():\n",
    "                print(f\"  - {feature}:\")\n",
    "                print(f\"    Mean: {stats['mean']:.2f}, Std: {stats['std']:.2f}, Min: {stats['min']:.2f}, Max: {stats['max']:.2f}\")\n",
    "\n",
    "            print(\"\\nCategorical Features:\")\n",
    "            for feature, categories in summary[\"categorical\"].items():\n",
    "                print(f\"  - {feature}:\")\n",
    "                for category, count in categories.items():\n",
    "                    print(f\"    {category}: {count}\")\n",
    "\n",
    "            return summary\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "            return None\n",
    "\n",
    "    # to test\n",
    "    def remove_quasi_constant_features(self, threshold=0.99): # function to test\n",
    "        \"\"\"\n",
    "        Detect and remove quasi-constant features from the dataset.\n",
    "\n",
    "        Args:\n",
    "            threshold (float): The threshold for detecting quasi-constant features.\n",
    "                               A feature is considered quasi-constant if the most\n",
    "                               frequent value appears in more than `threshold` proportion\n",
    "                               of the rows\n",
    "        \"\"\"\n",
    "        if self.data is not None and self.y is not None:\n",
    "            # Initialize a list to store quasi-constant features\n",
    "            quasi_constant_features = []\n",
    "\n",
    "            # Loop through features to calculate the proportion of the most frequent value\n",
    "            for feature in self.data.columns:\n",
    "                # Proportion of the most frequent value\n",
    "                most_frequent_value_ratio = self.data[feature].value_counts(normalize=True).max()\n",
    "                if most_frequent_value_ratio >= threshold:\n",
    "                    quasi_constant_features.append(feature)\n",
    "\n",
    "            if quasi_constant_features:\n",
    "                print(\"Detected quasi-constant features:\")\n",
    "                for feature in quasi_constant_features:\n",
    "                    print(f\"  - {feature} (most frequent value ratio: {most_frequent_value_ratio:.2f})\")\n",
    "\n",
    "                # Drop quasi-constant features\n",
    "                self.data.drop(columns=quasi_constant_features, inplace=True)\n",
    "                print(f\"Dropped {len(quasi_constant_features)} quasi-constant features.\")\n",
    "            else:\n",
    "                print(\"No quasi-constant features detected.\")\n",
    "\n",
    "   #\"\"\" these functions are used now to store the different log processes that have been done on the data\n",
    "   #this helps on knowing on which type of data we are working on\"\"\"\n",
    "\n",
    "\n",
    "    def log_processing_step(self, step_description):\n",
    "        \"\"\"\n",
    "        Log a processing step for tracking data transformations.\n",
    "        Args: step_description (str): A description of the processing step performed.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'log'):\n",
    "            self.log = []  # Initialize the log if it doesn't exist\n",
    "\n",
    "        # Append the step description with a timestamp\n",
    "        from datetime import datetime\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        log_entry = f\"[{timestamp}] {step_description}\"\n",
    "        self.log.append(log_entry)\n",
    "        print(f\"Logged step: {log_entry}\")\n",
    "\n",
    "    def show_logs(self):\n",
    "        \"\"\"\n",
    "        Display all logged processing steps.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'log') and self.log:\n",
    "            print(\"\\nProcessing Log:\")\n",
    "            for entry in self.log:\n",
    "                print(entry)\n",
    "        else:\n",
    "            print(\"No processing steps logged yet.\")\n",
    "\n",
    "    def save_logs(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the processing log to a file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the file where the log will be saved.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'log') and self.log:\n",
    "            try:\n",
    "                with open(filepath, 'w') as log_file:\n",
    "                    log_file.write(\"\\n\".join(self.log))\n",
    "                print(f\"Processing log saved to {filepath}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save log: {e}\")\n",
    "        else:\n",
    "            print(\"No processing steps logged yet.\")\n",
    "\n",
    "    def study_correlation_with_target(self, target=None, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Analyze the correlation of numerical features with the target variable.\n",
    "\n",
    "        Args:\n",
    "            target (pd.Series): The target variable (if separate from the dataset).\n",
    "                                If None, the class's `self.y` will be used.\n",
    "            threshold (float): Minimum absolute correlation value to consider a feature relevant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with features and their correlation with the target.\n",
    "        \"\"\"\n",
    "        if self.data is not None:\n",
    "            if target is None:\n",
    "                if hasattr(self, 'y'):\n",
    "                    target = self.y\n",
    "                else:\n",
    "                    print(\"Target variable not provided and `self.y` is undefined.\")\n",
    "                    return None\n",
    "\n",
    "            # Ensure the target is numeric for correlation computation\n",
    "            if not pd.api.types.is_numeric_dtype(target):\n",
    "                print(\"Target variable must be numeric for correlation analysis.\")\n",
    "                return None\n",
    "\n",
    "            # Compute correlations\n",
    "            numeric_features = self.data.select_dtypes(include=[\"number\"]).columns\n",
    "            correlation_results = {}\n",
    "\n",
    "            for feature in numeric_features:\n",
    "                corr = self.data[feature].corr(target)\n",
    "                correlation_results[feature] = corr\n",
    "\n",
    "            # Convert to DataFrame and filter by threshold\n",
    "            correlation_df = pd.DataFrame(list(correlation_results.items()), columns=[\"Feature\", \"Correlation\"])\n",
    "            correlation_df[\"Absolute Correlation\"] = correlation_df[\"Correlation\"].abs()\n",
    "            correlation_df = correlation_df.sort_values(by=\"Absolute Correlation\", ascending=False)\n",
    "\n",
    "            # Filter by threshold\n",
    "            relevant_features = correlation_df[correlation_df[\"Absolute Correlation\"] >= threshold]\n",
    "\n",
    "            # Print relevant features\n",
    "            print(f\"Features with absolute correlation >= {threshold}:\")\n",
    "            print(relevant_features)\n",
    "\n",
    "            return relevant_features\n",
    "        else:\n",
    "            print(\"No data loaded. Please load the data first.\")\n",
    "            return None\n",
    "\n",
    "    def export_preprocessed_data(self, main_folder=\"data\", subfolder_name=None, file_format=\"csv\"):\n",
    "        \"\"\"\n",
    "        Export the cleaned and transformed dataset, target variable, and logs to a new directory.\n",
    "\n",
    "        Args:\n",
    "            main_folder (str): The main directory where subfolders will be created for each export.\n",
    "            subfolder_name (str): Name of the subfolder for the current export. If None, a timestamp will be used.\n",
    "            file_format (str): File format for saving the dataset and target. Options: \"csv\" or \"excel\".\n",
    "        \"\"\"\n",
    "        # Create the main folder if it doesn't exist\n",
    "        if not os.path.exists(main_folder):\n",
    "            os.makedirs(main_folder)\n",
    "\n",
    "        # Generate subfolder name if not provided\n",
    "        if subfolder_name is None:\n",
    "            subfolder_name = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        export_path = os.path.join(main_folder, subfolder_name)\n",
    "\n",
    "        # Create the subfolder\n",
    "        if not os.path.exists(export_path):\n",
    "            os.makedirs(export_path)\n",
    "\n",
    "        try:\n",
    "            # Save the dataset\n",
    "            if self.data is not None:\n",
    "                if file_format == \"csv\":\n",
    "                    self.data.to_csv(os.path.join(export_path, \"processed_data.csv\"), index=False)\n",
    "                    print(f\"Dataset saved in '{export_path}/processed_data.csv'.\")\n",
    "                elif file_format == \"excel\":\n",
    "                    self.data.to_excel(os.path.join(export_path, \"processed_data.xlsx\"), index=False)\n",
    "                    print(f\"Dataset saved in '{export_path}/processed_data.xlsx'.\")\n",
    "                else:\n",
    "                    print(f\"Invalid file format '{file_format}'. Use 'csv' or 'excel'.\")\n",
    "            else:\n",
    "                print(\"No dataset available to export.\")\n",
    "\n",
    "            # Save the target variable\n",
    "            if hasattr(self, 'y') and self.y is not None:\n",
    "                if file_format == \"csv\":\n",
    "                    self.y.to_csv(os.path.join(export_path, \"processed_target.csv\"), index=False, header=[\"Target\"])\n",
    "                    print(f\"Target variable saved in '{export_path}/processed_target.csv'.\")\n",
    "                elif file_format == \"excel\":\n",
    "                    self.y.to_excel(os.path.join(export_path, \"processed_target.xlsx\"), index=False, header=[\"Target\"])\n",
    "                    print(f\"Target variable saved in '{export_path}/processed_target.xlsx'.\")\n",
    "\n",
    "            # Save the log\n",
    "            if hasattr(self, 'log') and self.log:\n",
    "                with open(os.path.join(export_path, \"processing_log.txt\"), \"w\") as log_file:\n",
    "                    log_file.write(\"\\n\".join(self.log))\n",
    "                print(f\"Processing log saved in '{export_path}/processing_log.txt'.\")\n",
    "            else:\n",
    "                print(\"No log available to export.\")\n",
    "\n",
    "            print(f\"Data export completed successfully. Files saved in '{export_path}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during export: {e}\")\n",
    "\n"
   ],
   "id": "c048eabc51ffd221",
   "outputs": [],
   "execution_count": 377
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:26:35.896913Z",
     "start_time": "2025-01-20T18:26:35.894602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "from datetime import datetime\n"
   ],
   "id": "75171aabd3456c7e",
   "outputs": [],
   "execution_count": 378
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:28.736005Z",
     "start_time": "2025-01-20T18:26:36.009040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Processor = Processor(filepath=\"../data/balanced_data.csv\")\n",
    "Processor.load_data()\n"
   ],
   "id": "b4a8b73abb07fc43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 379
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:28.770454Z",
     "start_time": "2025-01-20T18:28:28.767761Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.show_features()",
   "id": "c382bd11ca129aa0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the dataset:\n",
      "1. Flow ID\n",
      "2. SrcIP\n",
      "3. DstIP\n",
      "4. SrcPort\n",
      "5. DstPort\n",
      "6. Protocol\n",
      "7. mTimestampStart\n",
      "8. mTimestampLast\n",
      "9. Flow Duration\n",
      "10. Flow Bytes/s\n",
      "11. Flow Packets/s\n",
      "12. Tot Fwd Pkts\n",
      "13. Tot Bwd Pkts\n",
      "14. Total Length of Fwd Packet\n",
      "15. Total Length of Bwd Packet\n",
      "16. Fwd Packet Length Min\n",
      "17. Fwd Packet Length Max\n",
      "18. Fwd Packet Length Mean\n",
      "19. Fwd Packet Length Std\n",
      "20. Bwd Packet Length Min\n",
      "21. Bwd Packet Length Max\n",
      "22. Bwd Packet Length Mean\n",
      "23. Bwd Packet Length Std\n",
      "24. Flow IAT Mean\n",
      "25. Flow IAT Min\n",
      "26. Flow IAT Max\n",
      "27. Flow IAT Stddev\n",
      "28. Fwd IAT Min\n",
      "29. Fwd IAT Max\n",
      "30. Fwd IAT Mean\n",
      "31. Fwd IAT Std\n",
      "32. Fwd IAT Tot\n",
      "33. Bwd IAT Min\n",
      "34. Bwd IAT Max\n",
      "35. Bwd IAT Mean\n",
      "36. Bwd IAT Std\n",
      "37. Bwd IAT Tot\n",
      "38. Fwd PSH flags\n",
      "39. Bwd PSH flags\n",
      "40. Fwd URG flags\n",
      "41. Bwd URG flags\n",
      "42. Fwd Header Length\n",
      "43. Bwd Header Length\n",
      "44. Fwd Packets/s\n",
      "45. Bwd Packets/s\n",
      "46. Packet Length Min\n",
      "47. Packet Length Max\n",
      "48. Packet Length Mean\n",
      "49. Packet Length Std\n",
      "50. Packet Length Variance\n",
      "51. FIN Flag Cnt\n",
      "52. SYN Flag Cnt\n",
      "53. RST Flag Cnt\n",
      "54. PSH Flag Cnt\n",
      "55. ACK Flag Cnt\n",
      "56. URG Flag Cnt\n",
      "57. CWR Flag Cnt\n",
      "58. ECE Flag Cnt\n",
      "59. Down/Up Ratio\n",
      "60. Average Packet Size\n",
      "61. Fwd Segment Size Avg\n",
      "62. Bwd Segment Size Avg\n",
      "63. Fwd Bytes/Bulk Avg\n",
      "64. Fwd Packet/Bulk Avg\n",
      "65. Fwd Bulk Rate Avg\n",
      "66. Bwd Bytes/Bulk Avg\n",
      "67. Bwd Packet/Bulk Avg\n",
      "68. Bwd Bulk Rate Avg\n",
      "69. Subflow Fwd Packets\n",
      "70. Subflow Fwd Bytes\n",
      "71. Subflow Bwd Packets\n",
      "72. Subflow Bwd Bytes\n",
      "73. FWD Init Win Bytes\n",
      "74. Bwd Init Win Bytes\n",
      "75. Fwd Act Data Pkts\n",
      "76. Fwd Seg Size Min\n",
      "77. Active Min\n",
      "78. Active Mean\n",
      "79. Active Max\n",
      "80. Active Std\n",
      "81. Idle Min\n",
      "82. Idle Mean\n",
      "83. Idle Max\n",
      "84. Idle Std\n",
      "85. L3/L4 Protocol\n",
      "86. Int/Ext Dst IP\n",
      "87. Conn_state\n",
      "88. Service\n",
      "89. Label\n",
      "90. External_src\n",
      "91. External_dst\n",
      "92. Segment_src\n",
      "93. Segment_dst\n",
      "94. Expoid_src\n",
      "95. Expoid_dst\n",
      "\n",
      "Total features: 95\n"
     ]
    }
   ],
   "execution_count": 380
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:28.809335Z",
     "start_time": "2025-01-20T18:28:28.807736Z"
    }
   },
   "cell_type": "code",
   "source": "columns_to_drop  = ['Flow ID', 'SrcIP', 'DstIP','External_src', 'External_dst','Conn_state', 'Segment_src', 'Segment_dst', 'Expoid_src', 'Expoid_dst','mTimestampStart','mTimestampLast']",
   "id": "7e655dc2c57caf9a",
   "outputs": [],
   "execution_count": 381
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:29.276281Z",
     "start_time": "2025-01-20T18:28:28.820356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Processor.drop_features(columns_to_drop=columns_to_drop)\n",
    "Processor.show_features()"
   ],
   "id": "2756d8e5ea186fb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dropped the following columns: Segment_src, Expoid_dst, Expoid_src, External_dst, Segment_dst, Flow ID, DstIP, mTimestampStart, SrcIP, Conn_state, External_src, mTimestampLast\n",
      "Features in the dataset:\n",
      "1. SrcPort\n",
      "2. DstPort\n",
      "3. Protocol\n",
      "4. Flow Duration\n",
      "5. Flow Bytes/s\n",
      "6. Flow Packets/s\n",
      "7. Tot Fwd Pkts\n",
      "8. Tot Bwd Pkts\n",
      "9. Total Length of Fwd Packet\n",
      "10. Total Length of Bwd Packet\n",
      "11. Fwd Packet Length Min\n",
      "12. Fwd Packet Length Max\n",
      "13. Fwd Packet Length Mean\n",
      "14. Fwd Packet Length Std\n",
      "15. Bwd Packet Length Min\n",
      "16. Bwd Packet Length Max\n",
      "17. Bwd Packet Length Mean\n",
      "18. Bwd Packet Length Std\n",
      "19. Flow IAT Mean\n",
      "20. Flow IAT Min\n",
      "21. Flow IAT Max\n",
      "22. Flow IAT Stddev\n",
      "23. Fwd IAT Min\n",
      "24. Fwd IAT Max\n",
      "25. Fwd IAT Mean\n",
      "26. Fwd IAT Std\n",
      "27. Fwd IAT Tot\n",
      "28. Bwd IAT Min\n",
      "29. Bwd IAT Max\n",
      "30. Bwd IAT Mean\n",
      "31. Bwd IAT Std\n",
      "32. Bwd IAT Tot\n",
      "33. Fwd PSH flags\n",
      "34. Bwd PSH flags\n",
      "35. Fwd URG flags\n",
      "36. Bwd URG flags\n",
      "37. Fwd Header Length\n",
      "38. Bwd Header Length\n",
      "39. Fwd Packets/s\n",
      "40. Bwd Packets/s\n",
      "41. Packet Length Min\n",
      "42. Packet Length Max\n",
      "43. Packet Length Mean\n",
      "44. Packet Length Std\n",
      "45. Packet Length Variance\n",
      "46. FIN Flag Cnt\n",
      "47. SYN Flag Cnt\n",
      "48. RST Flag Cnt\n",
      "49. PSH Flag Cnt\n",
      "50. ACK Flag Cnt\n",
      "51. URG Flag Cnt\n",
      "52. CWR Flag Cnt\n",
      "53. ECE Flag Cnt\n",
      "54. Down/Up Ratio\n",
      "55. Average Packet Size\n",
      "56. Fwd Segment Size Avg\n",
      "57. Bwd Segment Size Avg\n",
      "58. Fwd Bytes/Bulk Avg\n",
      "59. Fwd Packet/Bulk Avg\n",
      "60. Fwd Bulk Rate Avg\n",
      "61. Bwd Bytes/Bulk Avg\n",
      "62. Bwd Packet/Bulk Avg\n",
      "63. Bwd Bulk Rate Avg\n",
      "64. Subflow Fwd Packets\n",
      "65. Subflow Fwd Bytes\n",
      "66. Subflow Bwd Packets\n",
      "67. Subflow Bwd Bytes\n",
      "68. FWD Init Win Bytes\n",
      "69. Bwd Init Win Bytes\n",
      "70. Fwd Act Data Pkts\n",
      "71. Fwd Seg Size Min\n",
      "72. Active Min\n",
      "73. Active Mean\n",
      "74. Active Max\n",
      "75. Active Std\n",
      "76. Idle Min\n",
      "77. Idle Mean\n",
      "78. Idle Max\n",
      "79. Idle Std\n",
      "80. L3/L4 Protocol\n",
      "81. Int/Ext Dst IP\n",
      "82. Service\n",
      "83. Label\n",
      "\n",
      "Total features: 83\n"
     ]
    }
   ],
   "execution_count": 382
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:29.513614Z",
     "start_time": "2025-01-20T18:28:29.280508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Processor.extract_target(target_column='Label')\n",
    "print(Processor.y.value_counts())\n",
    "Processor.show_features()"
   ],
   "id": "f11548d3e1c788bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column 'Label' extracted successfully and stored in self.y.\n",
      "Label\n",
      "1    1644599\n",
      "0    1644599\n",
      "Name: count, dtype: int64\n",
      "Features in the dataset:\n",
      "1. SrcPort\n",
      "2. DstPort\n",
      "3. Protocol\n",
      "4. Flow Duration\n",
      "5. Flow Bytes/s\n",
      "6. Flow Packets/s\n",
      "7. Tot Fwd Pkts\n",
      "8. Tot Bwd Pkts\n",
      "9. Total Length of Fwd Packet\n",
      "10. Total Length of Bwd Packet\n",
      "11. Fwd Packet Length Min\n",
      "12. Fwd Packet Length Max\n",
      "13. Fwd Packet Length Mean\n",
      "14. Fwd Packet Length Std\n",
      "15. Bwd Packet Length Min\n",
      "16. Bwd Packet Length Max\n",
      "17. Bwd Packet Length Mean\n",
      "18. Bwd Packet Length Std\n",
      "19. Flow IAT Mean\n",
      "20. Flow IAT Min\n",
      "21. Flow IAT Max\n",
      "22. Flow IAT Stddev\n",
      "23. Fwd IAT Min\n",
      "24. Fwd IAT Max\n",
      "25. Fwd IAT Mean\n",
      "26. Fwd IAT Std\n",
      "27. Fwd IAT Tot\n",
      "28. Bwd IAT Min\n",
      "29. Bwd IAT Max\n",
      "30. Bwd IAT Mean\n",
      "31. Bwd IAT Std\n",
      "32. Bwd IAT Tot\n",
      "33. Fwd PSH flags\n",
      "34. Bwd PSH flags\n",
      "35. Fwd URG flags\n",
      "36. Bwd URG flags\n",
      "37. Fwd Header Length\n",
      "38. Bwd Header Length\n",
      "39. Fwd Packets/s\n",
      "40. Bwd Packets/s\n",
      "41. Packet Length Min\n",
      "42. Packet Length Max\n",
      "43. Packet Length Mean\n",
      "44. Packet Length Std\n",
      "45. Packet Length Variance\n",
      "46. FIN Flag Cnt\n",
      "47. SYN Flag Cnt\n",
      "48. RST Flag Cnt\n",
      "49. PSH Flag Cnt\n",
      "50. ACK Flag Cnt\n",
      "51. URG Flag Cnt\n",
      "52. CWR Flag Cnt\n",
      "53. ECE Flag Cnt\n",
      "54. Down/Up Ratio\n",
      "55. Average Packet Size\n",
      "56. Fwd Segment Size Avg\n",
      "57. Bwd Segment Size Avg\n",
      "58. Fwd Bytes/Bulk Avg\n",
      "59. Fwd Packet/Bulk Avg\n",
      "60. Fwd Bulk Rate Avg\n",
      "61. Bwd Bytes/Bulk Avg\n",
      "62. Bwd Packet/Bulk Avg\n",
      "63. Bwd Bulk Rate Avg\n",
      "64. Subflow Fwd Packets\n",
      "65. Subflow Fwd Bytes\n",
      "66. Subflow Bwd Packets\n",
      "67. Subflow Bwd Bytes\n",
      "68. FWD Init Win Bytes\n",
      "69. Bwd Init Win Bytes\n",
      "70. Fwd Act Data Pkts\n",
      "71. Fwd Seg Size Min\n",
      "72. Active Min\n",
      "73. Active Mean\n",
      "74. Active Max\n",
      "75. Active Std\n",
      "76. Idle Min\n",
      "77. Idle Mean\n",
      "78. Idle Max\n",
      "79. Idle Std\n",
      "80. L3/L4 Protocol\n",
      "81. Int/Ext Dst IP\n",
      "82. Service\n",
      "\n",
      "Total features: 82\n"
     ]
    }
   ],
   "execution_count": 383
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:29.862113Z",
     "start_time": "2025-01-20T18:28:29.517544Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.detect_categorical(handle_nan=\"drop\")",
   "id": "529fa3e19a53c0b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features and their NaN Information:\n",
      "- Service:\n",
      "  NaN Count: 1384681\n",
      "  Percentage of NaNs: 42.10%\n",
      "  Action: Dropped rows with NaN in 'Service'.\n",
      "\n",
      "Categorical NaN handling completed.\n"
     ]
    }
   ],
   "execution_count": 384
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:29.872214Z",
     "start_time": "2025-01-20T18:28:29.866348Z"
    }
   },
   "cell_type": "code",
   "source": "print(Processor.y.value_counts())",
   "id": "72eacb173017b5e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    994268\n",
      "0    910249\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 385
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.423650Z",
     "start_time": "2025-01-20T18:28:29.887885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Processor.handle_duplicates()\n",
    "print(Processor.y.value_counts())"
   ],
   "id": "e925ccc46049dd50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 132596 duplicate rows in the dataset.\n",
      "Duplicate rows have been removed.\n",
      "Label\n",
      "0    909971\n",
      "1    861950\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 386
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.521517Z",
     "start_time": "2025-01-20T18:28:33.427858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(Processor.data.shape)\n",
    "Processor.encode_categorical(method=\"label\")\n",
    "print(Processor.data.shape)"
   ],
   "id": "74366e420793fd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1771921, 82)\n",
      "Encoding categorical features using label encoding.\n",
      "Feature 'Service' encoded using Label Encoding.\n",
      "Categorical encoding completed.\n",
      "(1771921, 82)\n"
     ]
    }
   ],
   "execution_count": 387
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.530400Z",
     "start_time": "2025-01-20T18:28:33.529122Z"
    }
   },
   "cell_type": "code",
   "source": "# correlated_features, _ = Processor.study_correlation(threshold=0.95)\n",
   "id": "92d014f461add8a6",
   "outputs": [],
   "execution_count": 388
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.535367Z",
     "start_time": "2025-01-20T18:28:33.534252Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "53ebe1e3001125cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.540469Z",
     "start_time": "2025-01-20T18:28:33.539005Z"
    }
   },
   "cell_type": "code",
   "source": "# Processor.show_features()",
   "id": "c47017877a2b276e",
   "outputs": [],
   "execution_count": 389
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.556383Z",
     "start_time": "2025-01-20T18:28:33.554774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply PCA to reduce to components explaining 95% of the variance\n",
    "# reduced_data = Processor.apply_pca(n_components=0.95, plot_variance=True)\n",
    "# Apply PCA to keep the first 2 components\n",
    "# reduced_data = Processor.apply_pca(n_components=2)\n",
    "\n",
    "# Processor.combine_correlated_features(correlated_features, method=\"mean\")\n"
   ],
   "id": "70d0e52e83d507c5",
   "outputs": [],
   "execution_count": 390
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.562604Z",
     "start_time": "2025-01-20T18:28:33.561323Z"
    }
   },
   "cell_type": "code",
   "source": "# Processor.show_features()",
   "id": "d0b7c11172cb7793",
   "outputs": [],
   "execution_count": 391
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.568641Z",
     "start_time": "2025-01-20T18:28:33.566556Z"
    }
   },
   "cell_type": "code",
   "source": "# Processor.combine_and_replace_correlated_features(correlated_features, method=\"mean\")",
   "id": "11f33b389de5170d",
   "outputs": [],
   "execution_count": 392
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.573711Z",
     "start_time": "2025-01-20T18:28:33.572485Z"
    }
   },
   "cell_type": "code",
   "source": "# Processor.show_features()",
   "id": "9328626fef2e6a60",
   "outputs": [],
   "execution_count": 393
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:33.855357Z",
     "start_time": "2025-01-20T18:28:33.577159Z"
    }
   },
   "cell_type": "code",
   "source": "num_issue_features = Processor.detect_missing_numerical()",
   "id": "995125fe08902fa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for numerical features:\n",
      "- Flow Bytes/s:\n",
      "  NaN Count: 15 (0.00%)\n",
      "  Inf Count: 712 (0.04%)\n",
      "- Flow Packets/s:\n",
      "  NaN Count: 0 (0.00%)\n",
      "  Inf Count: 727 (0.04%)\n",
      "- Flow IAT Mean:\n",
      "  NaN Count: 417 (0.02%)\n",
      "  Inf Count: 0 (0.00%)\n",
      "- Flow IAT Min:\n",
      "  NaN Count: 417 (0.02%)\n",
      "  Inf Count: 0 (0.00%)\n",
      "- Flow IAT Max:\n",
      "  NaN Count: 417 (0.02%)\n",
      "  Inf Count: 0 (0.00%)\n",
      "- Flow IAT Stddev:\n",
      "  NaN Count: 417 (0.02%)\n",
      "  Inf Count: 0 (0.00%)\n"
     ]
    }
   ],
   "execution_count": 394
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:34.666050Z",
     "start_time": "2025-01-20T18:28:33.859459Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.handle_missing_values(num_issue_features, method=\"drop\", custom_value=None)",
   "id": "eb5d65e75d4b8b1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with missing values in 'Flow Bytes/s'.\n",
      "Dropping rows with missing values in 'Flow Packets/s'.\n",
      "Dropping rows with missing values in 'Flow IAT Mean'.\n",
      "Dropping rows with missing values in 'Flow IAT Min'.\n",
      "Dropping rows with missing values in 'Flow IAT Max'.\n",
      "Dropping rows with missing values in 'Flow IAT Stddev'.\n",
      "Missing value handling completed.\n"
     ]
    }
   ],
   "execution_count": 395
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:34.672005Z",
     "start_time": "2025-01-20T18:28:34.670304Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.show_features()",
   "id": "dc19656403873e4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the dataset:\n",
      "1. SrcPort\n",
      "2. DstPort\n",
      "3. Protocol\n",
      "4. Flow Duration\n",
      "5. Flow Bytes/s\n",
      "6. Flow Packets/s\n",
      "7. Tot Fwd Pkts\n",
      "8. Tot Bwd Pkts\n",
      "9. Total Length of Fwd Packet\n",
      "10. Total Length of Bwd Packet\n",
      "11. Fwd Packet Length Min\n",
      "12. Fwd Packet Length Max\n",
      "13. Fwd Packet Length Mean\n",
      "14. Fwd Packet Length Std\n",
      "15. Bwd Packet Length Min\n",
      "16. Bwd Packet Length Max\n",
      "17. Bwd Packet Length Mean\n",
      "18. Bwd Packet Length Std\n",
      "19. Flow IAT Mean\n",
      "20. Flow IAT Min\n",
      "21. Flow IAT Max\n",
      "22. Flow IAT Stddev\n",
      "23. Fwd IAT Min\n",
      "24. Fwd IAT Max\n",
      "25. Fwd IAT Mean\n",
      "26. Fwd IAT Std\n",
      "27. Fwd IAT Tot\n",
      "28. Bwd IAT Min\n",
      "29. Bwd IAT Max\n",
      "30. Bwd IAT Mean\n",
      "31. Bwd IAT Std\n",
      "32. Bwd IAT Tot\n",
      "33. Fwd PSH flags\n",
      "34. Bwd PSH flags\n",
      "35. Fwd URG flags\n",
      "36. Bwd URG flags\n",
      "37. Fwd Header Length\n",
      "38. Bwd Header Length\n",
      "39. Fwd Packets/s\n",
      "40. Bwd Packets/s\n",
      "41. Packet Length Min\n",
      "42. Packet Length Max\n",
      "43. Packet Length Mean\n",
      "44. Packet Length Std\n",
      "45. Packet Length Variance\n",
      "46. FIN Flag Cnt\n",
      "47. SYN Flag Cnt\n",
      "48. RST Flag Cnt\n",
      "49. PSH Flag Cnt\n",
      "50. ACK Flag Cnt\n",
      "51. URG Flag Cnt\n",
      "52. CWR Flag Cnt\n",
      "53. ECE Flag Cnt\n",
      "54. Down/Up Ratio\n",
      "55. Average Packet Size\n",
      "56. Fwd Segment Size Avg\n",
      "57. Bwd Segment Size Avg\n",
      "58. Fwd Bytes/Bulk Avg\n",
      "59. Fwd Packet/Bulk Avg\n",
      "60. Fwd Bulk Rate Avg\n",
      "61. Bwd Bytes/Bulk Avg\n",
      "62. Bwd Packet/Bulk Avg\n",
      "63. Bwd Bulk Rate Avg\n",
      "64. Subflow Fwd Packets\n",
      "65. Subflow Fwd Bytes\n",
      "66. Subflow Bwd Packets\n",
      "67. Subflow Bwd Bytes\n",
      "68. FWD Init Win Bytes\n",
      "69. Bwd Init Win Bytes\n",
      "70. Fwd Act Data Pkts\n",
      "71. Fwd Seg Size Min\n",
      "72. Active Min\n",
      "73. Active Mean\n",
      "74. Active Max\n",
      "75. Active Std\n",
      "76. Idle Min\n",
      "77. Idle Mean\n",
      "78. Idle Max\n",
      "79. Idle Std\n",
      "80. L3/L4 Protocol\n",
      "81. Int/Ext Dst IP\n",
      "82. Service\n",
      "\n",
      "Total features: 82\n"
     ]
    }
   ],
   "execution_count": 396
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:34.687153Z",
     "start_time": "2025-01-20T18:28:34.685688Z"
    }
   },
   "cell_type": "code",
   "source": "print(Processor.y.shape, Processor.data.shape)\n",
   "id": "7b184e5a906d581a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1771194,) (1771194, 82)\n"
     ]
    }
   ],
   "execution_count": 397
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:35.703955Z",
     "start_time": "2025-01-20T18:28:34.698497Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.rescale_data(method=\"standardize\")",
   "id": "7de676c2eaad7587",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying standardization (mean=0, std=1).\n",
      "Rescaling completed using standardize.\n"
     ]
    }
   ],
   "execution_count": 398
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:35.709271Z",
     "start_time": "2025-01-20T18:28:35.708106Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8b506f46483d26f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:35.713993Z",
     "start_time": "2025-01-20T18:28:35.712887Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "afff4ef743f12167",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:35.722660Z",
     "start_time": "2025-01-20T18:28:35.721367Z"
    }
   },
   "cell_type": "code",
   "source": "# Processor.select_features_by_importance(threshold=0.01, model=None)",
   "id": "7323c4d3b375c3c7",
   "outputs": [],
   "execution_count": 399
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:35.736574Z",
     "start_time": "2025-01-20T18:28:35.734900Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.show_features()",
   "id": "9a9555ec904c721d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the dataset:\n",
      "1. SrcPort\n",
      "2. DstPort\n",
      "3. Protocol\n",
      "4. Flow Duration\n",
      "5. Flow Bytes/s\n",
      "6. Flow Packets/s\n",
      "7. Tot Fwd Pkts\n",
      "8. Tot Bwd Pkts\n",
      "9. Total Length of Fwd Packet\n",
      "10. Total Length of Bwd Packet\n",
      "11. Fwd Packet Length Min\n",
      "12. Fwd Packet Length Max\n",
      "13. Fwd Packet Length Mean\n",
      "14. Fwd Packet Length Std\n",
      "15. Bwd Packet Length Min\n",
      "16. Bwd Packet Length Max\n",
      "17. Bwd Packet Length Mean\n",
      "18. Bwd Packet Length Std\n",
      "19. Flow IAT Mean\n",
      "20. Flow IAT Min\n",
      "21. Flow IAT Max\n",
      "22. Flow IAT Stddev\n",
      "23. Fwd IAT Min\n",
      "24. Fwd IAT Max\n",
      "25. Fwd IAT Mean\n",
      "26. Fwd IAT Std\n",
      "27. Fwd IAT Tot\n",
      "28. Bwd IAT Min\n",
      "29. Bwd IAT Max\n",
      "30. Bwd IAT Mean\n",
      "31. Bwd IAT Std\n",
      "32. Bwd IAT Tot\n",
      "33. Fwd PSH flags\n",
      "34. Bwd PSH flags\n",
      "35. Fwd URG flags\n",
      "36. Bwd URG flags\n",
      "37. Fwd Header Length\n",
      "38. Bwd Header Length\n",
      "39. Fwd Packets/s\n",
      "40. Bwd Packets/s\n",
      "41. Packet Length Min\n",
      "42. Packet Length Max\n",
      "43. Packet Length Mean\n",
      "44. Packet Length Std\n",
      "45. Packet Length Variance\n",
      "46. FIN Flag Cnt\n",
      "47. SYN Flag Cnt\n",
      "48. RST Flag Cnt\n",
      "49. PSH Flag Cnt\n",
      "50. ACK Flag Cnt\n",
      "51. URG Flag Cnt\n",
      "52. CWR Flag Cnt\n",
      "53. ECE Flag Cnt\n",
      "54. Down/Up Ratio\n",
      "55. Average Packet Size\n",
      "56. Fwd Segment Size Avg\n",
      "57. Bwd Segment Size Avg\n",
      "58. Fwd Bytes/Bulk Avg\n",
      "59. Fwd Packet/Bulk Avg\n",
      "60. Fwd Bulk Rate Avg\n",
      "61. Bwd Bytes/Bulk Avg\n",
      "62. Bwd Packet/Bulk Avg\n",
      "63. Bwd Bulk Rate Avg\n",
      "64. Subflow Fwd Packets\n",
      "65. Subflow Fwd Bytes\n",
      "66. Subflow Bwd Packets\n",
      "67. Subflow Bwd Bytes\n",
      "68. FWD Init Win Bytes\n",
      "69. Bwd Init Win Bytes\n",
      "70. Fwd Act Data Pkts\n",
      "71. Fwd Seg Size Min\n",
      "72. Active Min\n",
      "73. Active Mean\n",
      "74. Active Max\n",
      "75. Active Std\n",
      "76. Idle Min\n",
      "77. Idle Mean\n",
      "78. Idle Max\n",
      "79. Idle Std\n",
      "80. L3/L4 Protocol\n",
      "81. Int/Ext Dst IP\n",
      "82. Service\n",
      "\n",
      "Total features: 82\n"
     ]
    }
   ],
   "execution_count": 400
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:38.255841Z",
     "start_time": "2025-01-20T18:28:35.751655Z"
    }
   },
   "cell_type": "code",
   "source": "summary = Processor.summarize_feature_distribution(top_categories=1)",
   "id": "12cbfcaa43daab3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Feature Distribution:\n",
      "\n",
      "Numerical Features:\n",
      "  - SrcPort:\n",
      "    Mean: -0.00, Std: 1.00, Min: -3.78, Max: 1.10\n",
      "  - DstPort:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.14, Max: 18.97\n",
      "  - Protocol:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.23, Max: 15.52\n",
      "  - Flow Duration:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.22, Max: 7.11\n",
      "  - Flow Bytes/s:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.09, Max: 184.45\n",
      "  - Flow Packets/s:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.09, Max: 45.22\n",
      "  - Tot Fwd Pkts:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.20, Max: 956.28\n",
      "  - Tot Bwd Pkts:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.13, Max: 813.33\n",
      "  - Total Length of Fwd Packet:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.21, Max: 537.97\n",
      "  - Total Length of Bwd Packet:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.05, Max: 865.73\n",
      "  - Fwd Packet Length Min:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.33, Max: 55.61\n",
      "  - Fwd Packet Length Max:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.22, Max: 2.58\n",
      "  - Fwd Packet Length Mean:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.98, Max: 30.21\n",
      "  - Fwd Packet Length Std:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.16, Max: 7.21\n",
      "  - Bwd Packet Length Min:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.58, Max: 12.61\n",
      "  - Bwd Packet Length Max:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.37, Max: 1.54\n",
      "  - Bwd Packet Length Mean:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.59, Max: 9.35\n",
      "  - Bwd Packet Length Std:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.20, Max: 3.80\n",
      "  - Flow IAT Mean:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.15, Max: 50.57\n",
      "  - Flow IAT Min:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.01, Max: 379.37\n",
      "  - Flow IAT Max:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.20, Max: 11.07\n",
      "  - Flow IAT Stddev:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.17, Max: 21.90\n",
      "  - Fwd IAT Min:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.02, Max: 76.20\n",
      "  - Fwd IAT Max:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.20, Max: 10.87\n",
      "  - Fwd IAT Mean:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.14, Max: 35.97\n",
      "  - Fwd IAT Std:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.17, Max: 16.59\n",
      "  - Fwd IAT Tot:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.22, Max: 7.12\n",
      "  - Bwd IAT Min:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.07, Max: 22.78\n",
      "  - Bwd IAT Max:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.19, Max: 11.39\n",
      "  - Bwd IAT Mean:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.12, Max: 20.39\n",
      "  - Bwd IAT Std:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.16, Max: 19.64\n",
      "  - Bwd IAT Tot:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.20, Max: 7.32\n",
      "  - Fwd PSH flags:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.04, Max: 27.27\n",
      "  - Bwd PSH flags:\n",
      "    Mean: 0.00, Std: 0.00, Min: 0.00, Max: 0.00\n",
      "  - Fwd URG flags:\n",
      "    Mean: 0.00, Std: 0.00, Min: 0.00, Max: 0.00\n",
      "  - Bwd URG flags:\n",
      "    Mean: 0.00, Std: 0.00, Min: 0.00, Max: 0.00\n",
      "  - Fwd Header Length:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.16, Max: 691.26\n",
      "  - Bwd Header Length:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.11, Max: 1009.26\n",
      "  - Fwd Packets/s:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.10, Max: 62.41\n",
      "  - Bwd Packets/s:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.08, Max: 47.50\n",
      "  - Packet Length Min:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.35, Max: 56.84\n",
      "  - Packet Length Max:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.40, Max: 1.52\n",
      "  - Packet Length Mean:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.94, Max: 19.96\n",
      "  - Packet Length Std:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.41, Max: 4.41\n",
      "  - Packet Length Variance:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.04, Max: 10.71\n",
      "  - FIN Flag Cnt:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.07, Max: 19.06\n",
      "  - SYN Flag Cnt:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.10, Max: 14.77\n",
      "  - RST Flag Cnt:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.86, Max: 45.97\n",
      "  - PSH Flag Cnt:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.31, Max: 452.64\n",
      "  - ACK Flag Cnt:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.22, Max: 1052.25\n",
      "  - URG Flag Cnt:\n",
      "    Mean: 0.00, Std: 0.00, Min: 0.00, Max: 0.00\n",
      "  - CWR Flag Cnt:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.50, Max: 4.49\n",
      "  - ECE Flag Cnt:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.50, Max: 4.49\n",
      "  - Down/Up Ratio:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.57, Max: 123.92\n",
      "  - Average Packet Size:\n",
      "    Mean: 0.00, Std: 1.00, Min: -2.07, Max: 29.08\n",
      "  - Fwd Segment Size Avg:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.98, Max: 30.21\n",
      "  - Bwd Segment Size Avg:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.59, Max: 9.35\n",
      "  - Fwd Bytes/Bulk Avg:\n",
      "    Mean: 0.00, Std: 0.00, Min: 0.00, Max: 0.00\n",
      "  - Fwd Packet/Bulk Avg:\n",
      "    Mean: 0.00, Std: 0.00, Min: 0.00, Max: 0.00\n",
      "  - Fwd Bulk Rate Avg:\n",
      "    Mean: 0.00, Std: 0.00, Min: 0.00, Max: 0.00\n",
      "  - Bwd Bytes/Bulk Avg:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.08, Max: 966.73\n",
      "  - Bwd Packet/Bulk Avg:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.26, Max: 1009.11\n",
      "  - Bwd Bulk Rate Avg:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.10, Max: 259.55\n",
      "  - Subflow Fwd Packets:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.03, Max: 1122.74\n",
      "  - Subflow Fwd Bytes:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.04, Max: 1076.38\n",
      "  - Subflow Bwd Packets:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.03, Max: 1001.97\n",
      "  - Subflow Bwd Bytes:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.01, Max: 1057.48\n",
      "  - FWD Init Win Bytes:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.10, Max: 0.91\n",
      "  - Bwd Init Win Bytes:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.11, Max: 70.12\n",
      "  - Fwd Act Data Pkts:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.21, Max: 1094.30\n",
      "  - Fwd Seg Size Min:\n",
      "    Mean: -0.00, Std: 1.00, Min: -1.22, Max: 3.49\n",
      "  - Active Min:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.06, Max: 157.94\n",
      "  - Active Mean:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.10, Max: 124.49\n",
      "  - Active Max:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.10, Max: 81.53\n",
      "  - Active Std:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.08, Max: 97.29\n",
      "  - Idle Min:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.19, Max: 11.66\n",
      "  - Idle Mean:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.19, Max: 11.43\n",
      "  - Idle Max:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.19, Max: 11.11\n",
      "  - Idle Std:\n",
      "    Mean: 0.00, Std: 1.00, Min: -0.07, Max: 53.68\n",
      "  - L3/L4 Protocol:\n",
      "    Mean: -0.00, Std: 1.00, Min: -0.74, Max: 6.96\n",
      "  - Int/Ext Dst IP:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.76, Max: 0.57\n",
      "  - Service:\n",
      "    Mean: 0.00, Std: 1.00, Min: -1.14, Max: 1.64\n",
      "\n",
      "Categorical Features:\n"
     ]
    }
   ],
   "execution_count": 401
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:38.261799Z",
     "start_time": "2025-01-20T18:28:38.259990Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.show_features()",
   "id": "5a1cc9db9670e591",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the dataset:\n",
      "1. SrcPort\n",
      "2. DstPort\n",
      "3. Protocol\n",
      "4. Flow Duration\n",
      "5. Flow Bytes/s\n",
      "6. Flow Packets/s\n",
      "7. Tot Fwd Pkts\n",
      "8. Tot Bwd Pkts\n",
      "9. Total Length of Fwd Packet\n",
      "10. Total Length of Bwd Packet\n",
      "11. Fwd Packet Length Min\n",
      "12. Fwd Packet Length Max\n",
      "13. Fwd Packet Length Mean\n",
      "14. Fwd Packet Length Std\n",
      "15. Bwd Packet Length Min\n",
      "16. Bwd Packet Length Max\n",
      "17. Bwd Packet Length Mean\n",
      "18. Bwd Packet Length Std\n",
      "19. Flow IAT Mean\n",
      "20. Flow IAT Min\n",
      "21. Flow IAT Max\n",
      "22. Flow IAT Stddev\n",
      "23. Fwd IAT Min\n",
      "24. Fwd IAT Max\n",
      "25. Fwd IAT Mean\n",
      "26. Fwd IAT Std\n",
      "27. Fwd IAT Tot\n",
      "28. Bwd IAT Min\n",
      "29. Bwd IAT Max\n",
      "30. Bwd IAT Mean\n",
      "31. Bwd IAT Std\n",
      "32. Bwd IAT Tot\n",
      "33. Fwd PSH flags\n",
      "34. Bwd PSH flags\n",
      "35. Fwd URG flags\n",
      "36. Bwd URG flags\n",
      "37. Fwd Header Length\n",
      "38. Bwd Header Length\n",
      "39. Fwd Packets/s\n",
      "40. Bwd Packets/s\n",
      "41. Packet Length Min\n",
      "42. Packet Length Max\n",
      "43. Packet Length Mean\n",
      "44. Packet Length Std\n",
      "45. Packet Length Variance\n",
      "46. FIN Flag Cnt\n",
      "47. SYN Flag Cnt\n",
      "48. RST Flag Cnt\n",
      "49. PSH Flag Cnt\n",
      "50. ACK Flag Cnt\n",
      "51. URG Flag Cnt\n",
      "52. CWR Flag Cnt\n",
      "53. ECE Flag Cnt\n",
      "54. Down/Up Ratio\n",
      "55. Average Packet Size\n",
      "56. Fwd Segment Size Avg\n",
      "57. Bwd Segment Size Avg\n",
      "58. Fwd Bytes/Bulk Avg\n",
      "59. Fwd Packet/Bulk Avg\n",
      "60. Fwd Bulk Rate Avg\n",
      "61. Bwd Bytes/Bulk Avg\n",
      "62. Bwd Packet/Bulk Avg\n",
      "63. Bwd Bulk Rate Avg\n",
      "64. Subflow Fwd Packets\n",
      "65. Subflow Fwd Bytes\n",
      "66. Subflow Bwd Packets\n",
      "67. Subflow Bwd Bytes\n",
      "68. FWD Init Win Bytes\n",
      "69. Bwd Init Win Bytes\n",
      "70. Fwd Act Data Pkts\n",
      "71. Fwd Seg Size Min\n",
      "72. Active Min\n",
      "73. Active Mean\n",
      "74. Active Max\n",
      "75. Active Std\n",
      "76. Idle Min\n",
      "77. Idle Mean\n",
      "78. Idle Max\n",
      "79. Idle Std\n",
      "80. L3/L4 Protocol\n",
      "81. Int/Ext Dst IP\n",
      "82. Service\n",
      "\n",
      "Total features: 82\n"
     ]
    }
   ],
   "execution_count": 402
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:39.719919Z",
     "start_time": "2025-01-20T18:28:38.279765Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.remove_quasi_constant_features(threshold=0.99)",
   "id": "8730a8caa4430c6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected quasi-constant features:\n",
      "  - Fwd PSH flags (most frequent value ratio: 0.38)\n",
      "  - Bwd PSH flags (most frequent value ratio: 0.38)\n",
      "  - Fwd URG flags (most frequent value ratio: 0.38)\n",
      "  - Bwd URG flags (most frequent value ratio: 0.38)\n",
      "  - URG Flag Cnt (most frequent value ratio: 0.38)\n",
      "  - Fwd Bytes/Bulk Avg (most frequent value ratio: 0.38)\n",
      "  - Fwd Packet/Bulk Avg (most frequent value ratio: 0.38)\n",
      "  - Fwd Bulk Rate Avg (most frequent value ratio: 0.38)\n",
      "Dropped 8 quasi-constant features.\n"
     ]
    }
   ],
   "execution_count": 403
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:39.725596Z",
     "start_time": "2025-01-20T18:28:39.724078Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.show_features()",
   "id": "dbfbca670c68fb16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the dataset:\n",
      "1. SrcPort\n",
      "2. DstPort\n",
      "3. Protocol\n",
      "4. Flow Duration\n",
      "5. Flow Bytes/s\n",
      "6. Flow Packets/s\n",
      "7. Tot Fwd Pkts\n",
      "8. Tot Bwd Pkts\n",
      "9. Total Length of Fwd Packet\n",
      "10. Total Length of Bwd Packet\n",
      "11. Fwd Packet Length Min\n",
      "12. Fwd Packet Length Max\n",
      "13. Fwd Packet Length Mean\n",
      "14. Fwd Packet Length Std\n",
      "15. Bwd Packet Length Min\n",
      "16. Bwd Packet Length Max\n",
      "17. Bwd Packet Length Mean\n",
      "18. Bwd Packet Length Std\n",
      "19. Flow IAT Mean\n",
      "20. Flow IAT Min\n",
      "21. Flow IAT Max\n",
      "22. Flow IAT Stddev\n",
      "23. Fwd IAT Min\n",
      "24. Fwd IAT Max\n",
      "25. Fwd IAT Mean\n",
      "26. Fwd IAT Std\n",
      "27. Fwd IAT Tot\n",
      "28. Bwd IAT Min\n",
      "29. Bwd IAT Max\n",
      "30. Bwd IAT Mean\n",
      "31. Bwd IAT Std\n",
      "32. Bwd IAT Tot\n",
      "33. Fwd Header Length\n",
      "34. Bwd Header Length\n",
      "35. Fwd Packets/s\n",
      "36. Bwd Packets/s\n",
      "37. Packet Length Min\n",
      "38. Packet Length Max\n",
      "39. Packet Length Mean\n",
      "40. Packet Length Std\n",
      "41. Packet Length Variance\n",
      "42. FIN Flag Cnt\n",
      "43. SYN Flag Cnt\n",
      "44. RST Flag Cnt\n",
      "45. PSH Flag Cnt\n",
      "46. ACK Flag Cnt\n",
      "47. CWR Flag Cnt\n",
      "48. ECE Flag Cnt\n",
      "49. Down/Up Ratio\n",
      "50. Average Packet Size\n",
      "51. Fwd Segment Size Avg\n",
      "52. Bwd Segment Size Avg\n",
      "53. Bwd Bytes/Bulk Avg\n",
      "54. Bwd Packet/Bulk Avg\n",
      "55. Bwd Bulk Rate Avg\n",
      "56. Subflow Fwd Packets\n",
      "57. Subflow Fwd Bytes\n",
      "58. Subflow Bwd Packets\n",
      "59. Subflow Bwd Bytes\n",
      "60. FWD Init Win Bytes\n",
      "61. Bwd Init Win Bytes\n",
      "62. Fwd Act Data Pkts\n",
      "63. Fwd Seg Size Min\n",
      "64. Active Min\n",
      "65. Active Mean\n",
      "66. Active Max\n",
      "67. Active Std\n",
      "68. Idle Min\n",
      "69. Idle Mean\n",
      "70. Idle Max\n",
      "71. Idle Std\n",
      "72. L3/L4 Protocol\n",
      "73. Int/Ext Dst IP\n",
      "74. Service\n",
      "\n",
      "Total features: 74\n"
     ]
    }
   ],
   "execution_count": 404
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:40.807077Z",
     "start_time": "2025-01-20T18:28:39.799311Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.study_correlation_with_target(target=None, threshold=0.1)",
   "id": "994fc88884dd8084",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with absolute correlation >= 0.1:\n",
      "                   Feature  Correlation  Absolute Correlation\n",
      "41            FIN Flag Cnt     0.858779              0.858779\n",
      "59      FWD Init Win Bytes     0.844482              0.844482\n",
      "43            RST Flag Cnt     0.819852              0.819852\n",
      "42            SYN Flag Cnt     0.814564              0.814564\n",
      "13   Fwd Packet Length Std     0.812910              0.812910\n",
      "11   Fwd Packet Length Max     0.794353              0.794353\n",
      "17   Bwd Packet Length Std     0.778323              0.778323\n",
      "15   Bwd Packet Length Max     0.723488              0.723488\n",
      "37       Packet Length Max     0.721289              0.721289\n",
      "71          L3/L4 Protocol    -0.718759              0.718759\n",
      "39       Packet Length Std     0.690233              0.690233\n",
      "2                 Protocol    -0.571241              0.571241\n",
      "72          Int/Ext Dst IP     0.554426              0.554426\n",
      "40  Packet Length Variance     0.545131              0.545131\n",
      "48           Down/Up Ratio    -0.543081              0.543081\n",
      "62        Fwd Seg Size Min     0.532748              0.532748\n",
      "50    Fwd Segment Size Avg     0.503814              0.503814\n",
      "12  Fwd Packet Length Mean     0.503814              0.503814\n",
      "46            CWR Flag Cnt     0.501754              0.501754\n",
      "47            ECE Flag Cnt     0.500778              0.500778\n",
      "16  Bwd Packet Length Mean     0.456796              0.456796\n",
      "51    Bwd Segment Size Avg     0.456796              0.456796\n",
      "36       Packet Length Min    -0.417262              0.417262\n",
      "10   Fwd Packet Length Min    -0.416213              0.416213\n",
      "73                 Service     0.414455              0.414455\n",
      "38      Packet Length Mean     0.400743              0.400743\n",
      "14   Bwd Packet Length Min    -0.355287              0.355287\n",
      "49     Average Packet Size     0.302785              0.302785\n",
      "0                  SrcPort     0.194709              0.194709\n",
      "23             Fwd IAT Max    -0.194045              0.194045\n",
      "20            Flow IAT Max    -0.193426              0.193426\n",
      "3            Flow Duration    -0.193102              0.193102\n",
      "26             Fwd IAT Tot    -0.192735              0.192735\n",
      "69                Idle Max    -0.181656              0.181656\n",
      "68               Idle Mean    -0.180406              0.180406\n",
      "28             Bwd IAT Max    -0.179368              0.179368\n",
      "31             Bwd IAT Tot    -0.179158              0.179158\n",
      "67                Idle Min    -0.177611              0.177611\n",
      "21         Flow IAT Stddev    -0.162939              0.162939\n",
      "25             Fwd IAT Std    -0.159876              0.159876\n",
      "30             Bwd IAT Std    -0.150458              0.150458\n",
      "18           Flow IAT Mean    -0.143159              0.143159\n",
      "24            Fwd IAT Mean    -0.135087              0.135087\n",
      "53     Bwd Packet/Bulk Avg     0.118464              0.118464\n",
      "29            Bwd IAT Mean    -0.117171              0.117171\n",
      "45            ACK Flag Cnt     0.110515              0.110515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                   Feature  Correlation  Absolute Correlation\n",
       "41            FIN Flag Cnt     0.858779              0.858779\n",
       "59      FWD Init Win Bytes     0.844482              0.844482\n",
       "43            RST Flag Cnt     0.819852              0.819852\n",
       "42            SYN Flag Cnt     0.814564              0.814564\n",
       "13   Fwd Packet Length Std     0.812910              0.812910\n",
       "11   Fwd Packet Length Max     0.794353              0.794353\n",
       "17   Bwd Packet Length Std     0.778323              0.778323\n",
       "15   Bwd Packet Length Max     0.723488              0.723488\n",
       "37       Packet Length Max     0.721289              0.721289\n",
       "71          L3/L4 Protocol    -0.718759              0.718759\n",
       "39       Packet Length Std     0.690233              0.690233\n",
       "2                 Protocol    -0.571241              0.571241\n",
       "72          Int/Ext Dst IP     0.554426              0.554426\n",
       "40  Packet Length Variance     0.545131              0.545131\n",
       "48           Down/Up Ratio    -0.543081              0.543081\n",
       "62        Fwd Seg Size Min     0.532748              0.532748\n",
       "50    Fwd Segment Size Avg     0.503814              0.503814\n",
       "12  Fwd Packet Length Mean     0.503814              0.503814\n",
       "46            CWR Flag Cnt     0.501754              0.501754\n",
       "47            ECE Flag Cnt     0.500778              0.500778\n",
       "16  Bwd Packet Length Mean     0.456796              0.456796\n",
       "51    Bwd Segment Size Avg     0.456796              0.456796\n",
       "36       Packet Length Min    -0.417262              0.417262\n",
       "10   Fwd Packet Length Min    -0.416213              0.416213\n",
       "73                 Service     0.414455              0.414455\n",
       "38      Packet Length Mean     0.400743              0.400743\n",
       "14   Bwd Packet Length Min    -0.355287              0.355287\n",
       "49     Average Packet Size     0.302785              0.302785\n",
       "0                  SrcPort     0.194709              0.194709\n",
       "23             Fwd IAT Max    -0.194045              0.194045\n",
       "20            Flow IAT Max    -0.193426              0.193426\n",
       "3            Flow Duration    -0.193102              0.193102\n",
       "26             Fwd IAT Tot    -0.192735              0.192735\n",
       "69                Idle Max    -0.181656              0.181656\n",
       "68               Idle Mean    -0.180406              0.180406\n",
       "28             Bwd IAT Max    -0.179368              0.179368\n",
       "31             Bwd IAT Tot    -0.179158              0.179158\n",
       "67                Idle Min    -0.177611              0.177611\n",
       "21         Flow IAT Stddev    -0.162939              0.162939\n",
       "25             Fwd IAT Std    -0.159876              0.159876\n",
       "30             Bwd IAT Std    -0.150458              0.150458\n",
       "18           Flow IAT Mean    -0.143159              0.143159\n",
       "24            Fwd IAT Mean    -0.135087              0.135087\n",
       "53     Bwd Packet/Bulk Avg     0.118464              0.118464\n",
       "29            Bwd IAT Mean    -0.117171              0.117171\n",
       "45            ACK Flag Cnt     0.110515              0.110515"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Absolute Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>FIN Flag Cnt</td>\n",
       "      <td>0.858779</td>\n",
       "      <td>0.858779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>FWD Init Win Bytes</td>\n",
       "      <td>0.844482</td>\n",
       "      <td>0.844482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RST Flag Cnt</td>\n",
       "      <td>0.819852</td>\n",
       "      <td>0.819852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SYN Flag Cnt</td>\n",
       "      <td>0.814564</td>\n",
       "      <td>0.814564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fwd Packet Length Std</td>\n",
       "      <td>0.812910</td>\n",
       "      <td>0.812910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fwd Packet Length Max</td>\n",
       "      <td>0.794353</td>\n",
       "      <td>0.794353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bwd Packet Length Std</td>\n",
       "      <td>0.778323</td>\n",
       "      <td>0.778323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bwd Packet Length Max</td>\n",
       "      <td>0.723488</td>\n",
       "      <td>0.723488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Packet Length Max</td>\n",
       "      <td>0.721289</td>\n",
       "      <td>0.721289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>L3/L4 Protocol</td>\n",
       "      <td>-0.718759</td>\n",
       "      <td>0.718759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Packet Length Std</td>\n",
       "      <td>0.690233</td>\n",
       "      <td>0.690233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protocol</td>\n",
       "      <td>-0.571241</td>\n",
       "      <td>0.571241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Int/Ext Dst IP</td>\n",
       "      <td>0.554426</td>\n",
       "      <td>0.554426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Packet Length Variance</td>\n",
       "      <td>0.545131</td>\n",
       "      <td>0.545131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Down/Up Ratio</td>\n",
       "      <td>-0.543081</td>\n",
       "      <td>0.543081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Fwd Seg Size Min</td>\n",
       "      <td>0.532748</td>\n",
       "      <td>0.532748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Fwd Segment Size Avg</td>\n",
       "      <td>0.503814</td>\n",
       "      <td>0.503814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fwd Packet Length Mean</td>\n",
       "      <td>0.503814</td>\n",
       "      <td>0.503814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>CWR Flag Cnt</td>\n",
       "      <td>0.501754</td>\n",
       "      <td>0.501754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ECE Flag Cnt</td>\n",
       "      <td>0.500778</td>\n",
       "      <td>0.500778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bwd Packet Length Mean</td>\n",
       "      <td>0.456796</td>\n",
       "      <td>0.456796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Bwd Segment Size Avg</td>\n",
       "      <td>0.456796</td>\n",
       "      <td>0.456796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Packet Length Min</td>\n",
       "      <td>-0.417262</td>\n",
       "      <td>0.417262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fwd Packet Length Min</td>\n",
       "      <td>-0.416213</td>\n",
       "      <td>0.416213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Service</td>\n",
       "      <td>0.414455</td>\n",
       "      <td>0.414455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Packet Length Mean</td>\n",
       "      <td>0.400743</td>\n",
       "      <td>0.400743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bwd Packet Length Min</td>\n",
       "      <td>-0.355287</td>\n",
       "      <td>0.355287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Average Packet Size</td>\n",
       "      <td>0.302785</td>\n",
       "      <td>0.302785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SrcPort</td>\n",
       "      <td>0.194709</td>\n",
       "      <td>0.194709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Fwd IAT Max</td>\n",
       "      <td>-0.194045</td>\n",
       "      <td>0.194045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Flow IAT Max</td>\n",
       "      <td>-0.193426</td>\n",
       "      <td>0.193426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flow Duration</td>\n",
       "      <td>-0.193102</td>\n",
       "      <td>0.193102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fwd IAT Tot</td>\n",
       "      <td>-0.192735</td>\n",
       "      <td>0.192735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Idle Max</td>\n",
       "      <td>-0.181656</td>\n",
       "      <td>0.181656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Idle Mean</td>\n",
       "      <td>-0.180406</td>\n",
       "      <td>0.180406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bwd IAT Max</td>\n",
       "      <td>-0.179368</td>\n",
       "      <td>0.179368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Bwd IAT Tot</td>\n",
       "      <td>-0.179158</td>\n",
       "      <td>0.179158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Idle Min</td>\n",
       "      <td>-0.177611</td>\n",
       "      <td>0.177611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Flow IAT Stddev</td>\n",
       "      <td>-0.162939</td>\n",
       "      <td>0.162939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fwd IAT Std</td>\n",
       "      <td>-0.159876</td>\n",
       "      <td>0.159876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bwd IAT Std</td>\n",
       "      <td>-0.150458</td>\n",
       "      <td>0.150458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Flow IAT Mean</td>\n",
       "      <td>-0.143159</td>\n",
       "      <td>0.143159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fwd IAT Mean</td>\n",
       "      <td>-0.135087</td>\n",
       "      <td>0.135087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Bwd Packet/Bulk Avg</td>\n",
       "      <td>0.118464</td>\n",
       "      <td>0.118464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bwd IAT Mean</td>\n",
       "      <td>-0.117171</td>\n",
       "      <td>0.117171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ACK Flag Cnt</td>\n",
       "      <td>0.110515</td>\n",
       "      <td>0.110515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 405
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:40.880316Z",
     "start_time": "2025-01-20T18:28:40.878976Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "66d93fdfd96bcee3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:29:43.054979Z",
     "start_time": "2025-01-20T18:28:40.985636Z"
    }
   },
   "cell_type": "code",
   "source": "Processor.export_preprocessed_data( main_folder=\"../data\", subfolder_name=\"test_1\", file_format=\"csv\")",
   "id": "bf7fabb6d81f50b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved in '../data/test_1/processed_data.csv'.\n",
      "Target variable saved in '../data/test_1/processed_target.csv'.\n",
      "No log available to export.\n",
      "Data export completed successfully. Files saved in '../data/test_1'.\n"
     ]
    }
   ],
   "execution_count": 406
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:40.946249Z",
     "start_time": "2025-01-20T18:28:40.944929Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f8f2fae23b2e90e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T18:28:40.973785Z",
     "start_time": "2025-01-20T18:28:40.972270Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e6ac655f53dcd17b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "98dc5779e17be9f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
